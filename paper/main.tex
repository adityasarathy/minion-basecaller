\documentclass[times, utf8, seminar, numeric]{fer}
\usepackage{booktabs}
\usepackage{mathtools, amsmath,amsfonts,amssymb, amsthm}
\usepackage{acronym}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {Figures/} }

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\begin{document}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\voditelj{izv.~prof.~dr.~sc.~Mile Sikić}
\title{MinCall --- MinION od raw signala do genoma}
\engtitle{MinCall --- MinION end2end deep learning basecaller}
\author{Neven Miculinić}

\maketitle
\tableofcontents

\begin{sazetak}
    U ovom znanstvenom clanku istenirali smo model dubokog ucenja za Oxford Nanopore Tehnology's MinION portabilni DNK sekvencor. MinION potencijal je ogroman, i zbog njegove male velicina i portabilnosti, kao i dugih ocitanja senzora (ocitanja > 100kbP su primjecena). Nazalost pati od vecih gresaka u usporedbi s drugim modelima.

    Kreirali smo nezavisni model pocevsi od izvornih podataka i koristili konvolucijske neuronske mreze (CNN) s CTC gubitkom te postigli mjerljive perfomanse.

\kljucnerijeci{Basecaller, MinION, R9, CNN, CTC, sekvenciniranje sljedece generacije}
\end{sazetak}

\begin{abstract}
    In this study we create end2end deep learning model for Oxford Nanopore Technology's MinION portable DNA sequencing device. The device's potential is enormous, both due to it's small size and portability and long read (over 100kBp reported). However is suffers with higher error rate compared to other methods.

    We've created independant model starting with raw data and used Convolutional Neural Network (CNN) combined with Connectionist Temporal Classification (CTC) loss for measurable performance.
\keywords{Basecaller, MinION, R9, CNN, CTC, Next generation sequecing}
\end{abstract}

\chapter{Introduction}

In this paper, we introduce the base caller for the MinION nanopore sequencing platform~\cite{mikheyev2014first}. The MinION device by Oxford Nanopore, weighing only 90 grams, is currently the smallest high-throughput DNA sequencer. Thanks to its low capital costs, small size and the possibility of analyzing the data in real time as they are produced, MinION is very promising for clinical applications, such as monitoring infectious disease outbreaks~\cite{judge2015early}\cite{quick2016real}, characterizing structural variants in cancer\cite{norris2016nanopore} and even full human genome assembly~\cite{jain2017nanopore}.

Although MinION is able to produce long reads, they have a high sequencing error rate. This has been somewhat alleviated with new R9 pore model, replacing older R7 ones. In this paper, we show that this error rate can be reduced by replacing the default base caller provided by the manufacturer with a properly trained neural network model.

In the MinION device, single-stranded DNA fragments move through nanopores, which causes drops in the electric current. The electric current is measured at each pore several thousand times per second. The electric current depends mostly on the context of several DNA bases passing through the pore at the time of measurement. As the DNA moves through the pore, the context shifts, and the electric current changes.

A MinION device typically yields reads several thousand bases long; reads as long as 100,000 bp have been reported. 1D read has usually error rate about   10\%. (Check details!!, What is the error rate, how it's defined???) After parameter training, base calling can be performed by running the Viterbi algorithm, which will result in the sequence of states with the highest likelihood.

The exact error rate metric is unreliable since multiple pipline tools could be the issue. First the sample is prepared, hopefully uncontaminated and matching reference genome as close as possile, then sequenced using the MinION device obtaining raw data. Next our model (or other groups ones) come along, basecall the sequence. To evalute error rate metric basecalled read is aligned to reference using aligners with their own erros/biases, mostly commonly used BWA-MEM and Graphmap.

\chapter{Background}

\section{Sequencing overview}
Informally, the MinION sequencer works as follows. First, DNA is sheared into fragments of 8–20 kbp and adapters are ligated to either end of the fragments. The resulting DNA fragments pass through a protein embedded in a membrane via a nanometre-sized channel (this protein is the `nanopore'). A single strand of DNA passes through the pore; the optional use of a hairpin adapter at one end of the fragment allows the two strands of DNA to serially pass through the nanopore, allowing two measurements of the fragment. With hairpin, it's called 2D read, which isn't the topic of this paper. In here we focus only on 1D reads.

In ONT terminology, the first strand going through the nanopore is the template, and the second is the complement. As a DNA strand passes through the pore it partially blocks the flow of electric current through the pore. The flow of current is sampled over time which is the observable output of the system. The central idea is that the single-stranded DNA product present in the nanopore affects the current in a way that is strong enough to enable decoding the electric signal data into a DNA sequence. This process, called basecalling, takes as input a list of current measurements and produces as output a list of DNA bases most likely to have generated those currents.

The nanopore is 6 nucleotides wide, and eariler HMM-based models model it as such.

\section{Basecalling}

The core of the decoding process is the basecalling step. Official basecaller is Metrichor, previously performed in the cloud before being open-sourced under name Nanonet.

Earlier models were (Hidden Markov model) HMM-based where hidden state modeled DNA sequence of length 6 (6-mer) in the nanopore. Pore models were used in computing emission probabilities.~\cite{loman2015complete,schreiber2015analysis,szalay2015novo,timp2012dna} and the recent open source HMM-based basecaller Nanocall~\cite{david2016nanocall}.

Recent models opted using (Recurrent neural network) RNN, notably DeepNano~\cite{deepnano} and recently open sourced official Nanonets from ONT.

\chapter{Method}

Instead of opting for the traditional path using HMM or RNN we tried using CNN~(Convolutional neural networks)~\cite{lecun-98}, that is their residual version~\cite{he2016deep}. We opted out for gated residual network variant~\cite{savarese2016learning}. For loss we used CTC (Connectionist temporal classification)~\cite{graves2006connectionist} between basecallled and the target sequence. The implementation used is open source warp-ctc~\cite{warpctc}. Main computation framework is tensorflow~\cite{tensorflow2015-whitepaper}

\section{Data preprocessing}

Dataset was obtained from (Ask somebody, Marko doesn't know!). In this research, models were trained on the ecoli K-12 strand. (Should I cite someone for strand). Data was split into train and test subset, such that aligned reads map to different reference genome parts. For initial model data bootstrapping metrichorn was used, version (Ask Someone?).

The fast5 input training files were further split into smaller training blocks, consisting of fixed block size on raw signal. For each block target sequence, basecalled data is used in following way.

We're using basecalled knowledge which tells us on each raw read part which 6-mer were currently in the nanopore. Using this data, we get the intermediate target sequence for each block. To correct for model errors, we use aligned information, that is reference genome and alignment data(cigar string) to correct that information, yielding finalized target sequence. For further performance we skip first and last training block, since most error aggregate on edges.

Adjacent matching bases are separated with surrogate one, for example AAA -> AA'A for reasons described in the following section.
\section{Residual arhitecture}

We use gated residual network, that is layer function is like this: $f(x) = k g(x) + x$ Where $x$ is input layer and $k$ constant learnt during training. Specifically, $g$ used is multiple, 1--3 times in our experiments, Relu-BatchNorm-CNN layers.

\section{CTC}

This section gives a brief overview over CTC, for further detail and in depth explanation we recommend original paper~\cite{graves2006connectionist} or searching for contemporary blog posts.

In CTC we have target sequecne $\mathbf{l}$ consisting from symbols of alphabet $\Sigma$. Our models outputs $\mathbf{x}$, and CTC loss maximizes $p(\mathbf{l}|\mathbf{x})$.

$\mathbf{x}$ is consistend of $n$ independent discrete random variables, $X_i$  over domain $\Sigma \cup \text{Blank}$. Path $p$ through $\mathbf{x}$ is assigning for each $X_i$ single value. It's log probabity is $\log P(p) = \sum_i {\log P(X_i=x_i)}$ where $x_i$s define the path.

We further have merging operator on path, $\text{merge}(x_1, x_2, \ldots, x_n)$ which merges together adjacent equivalent elements. For example $\text{merge}(AAAGC) = AGC$.

Then $p(\mathbf{l}|\mathbf{x}) = \sum_{p\in\text{paths}(\mathbf{x}, \text{merged}(p) = \mathbf{l}}{P(p)}$ or less formally, $P(\mathbf{l}|\mathbf{x})$ is sum of all path probabilites which when merged give $\mathbf{l}$

In out concrete $x$ application $\Sigma = \{\text{A G T C A' G' T' A'}\}$

For decoding, we use beam search decoder, with beam width 100.

\chapter{Results}

TODO

See \url{https://github.com/nmiculinic/minion-basecaller/blob/master/notebook/Lambda_nanonet_vs_m270.ipynb for preliminary results.}

\chapter{Conclusion and further work}

This model used advance state-of-the-art gated residual convolutional neural network, with top models having 270 layers and over 3M parameters, yet improvements over Metrichorn baseline are marginal. As the conclusion, it might be that we've reached Bayesian error rate for R9 chemistry. Furthermore, R9.5 and 1D\^2 reads are under development which shall yield this paper's result obsolete quite soon, yet underlying code developed could easily be adjusted and trained on new data.

Unlike Nanonet which uses custom OpenCL kernels or Albacore --- a novel ONT basecaller as of May 2017 lacking GPU support, this work used world-class computational framework tensorflow with highly optimized kernels and large development community. Therefore resulting paper's effect is showcasing Residual CNN approach or pure CNN approach with CTC loss is marginally better than already established basecaller and providing code in the contemporary framework.

\chapter{Acknowledgments}

TODO.

\bibliographystyle{fer}
\bibliography{refs}

\end{document}
end{document}
