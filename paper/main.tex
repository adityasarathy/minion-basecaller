%!TeX spellcheck = en-US
\documentclass[times, utf8, seminar, numeric]{fer}
\usepackage{booktabs}
\usepackage{mathtools, amsmath,amsfonts,amssymb, amsthm}
\usepackage{acronym}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {Figures/} }

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\begin{document}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\voditelj{Mile Sikić}
\title{MinCall --- MinION od raw signala do genoma}
\engtitle{MinCall --- MinION end2end deep learning basecaller}
\author{Neven Miculinić}

\maketitle
\tableofcontents

\begin{sazetak}
MinION je citac DNA najnovije generacije. Oxford Nanopore Technologies ga je u svrhu prvog portabilnog DNA sekvencora. Proizvodi vrlo duga ocitanja (duljine i preko 100 kBp), doduse pod cijelu preciznosti. U ovom radu prezentiramo altrenativnu metodu ocitanja DNA koristenjem convolucijskim neuronskih mreza (CNN) umjesto dosadasnjeg pristupa s HMM ili RNN. Takoder cijeli postupak je istreniran od izvornog signala do ocitanih nukleoida, bez ovisnosti o umjetnoj segregaciji na dogadaje.

\kljucnerijeci{Basecaller, MinION, R9, CNN, CTC, sekvenciniranje sljedece generacije}
\end{sazetak}

\begin{abstract}
    The MinION device by Oxford Nanopore is the first portable sequencing device. MinION is able to produce very long reads (reads over 100 kBp were reported), however it suffers from high sequencing error rate. In this paper, we show novel model basecaller which basecalles 1D reads using gated residual convolutional neural networks (CNN) combined with Connectionist Temporal Classification (CTC) learning approach. Its performance is on par with state of the art and offers possible future improvement.

    Whole process is trainned end-to-end from raw signal on R9 sequecing pore and once trainned it's independent from Metrichor.
\keywords{Basecaller, MinION, R9, CNN, CTC, Next generation sequecing}
\end{abstract}

\chapter{Introduction}

In this paper, we introduce the base caller for the MinION nanopore sequencing platform~\cite{mikheyev2014first}. The MinION device by Oxford Nanopore, weighing only 90 grams, is currently the smallest high-throughput DNA sequencer. Thanks to its low capital costs, small size and the possibility of analyzing the data in real time as they are produced, MinION is very promising for clinical applications, such as monitoring infectious disease outbreaks~\cite{judge2015early}\cite{quick2016real}, characterizing structural variants in cancer\cite{norris2016nanopore} and even full human genome assembly~\cite{jain2017nanopore}.

Although MinION is able to produce long reads, they have a high sequencing error rate. This has been somewhat aleviated with new R9 pore model, replacing older R7 ones. In this paper, we show that this error rate can be reduced by replacing the default base caller provided by the manufacturer with a properly trained neural network model.

In the MinION device, single-stranded DNA fragments move through nanopores, which causes drops in the electric current. The electric current is measured at each pore several thousand times per second. The electric current depends mostly on the context of several DNA bases passing through the pore at the time of measurement. As the DNA moves through the pore, the context shifts and the electric current changes.

A MinION device typically yields reads several thousand bases long; reads as long as 100,000 bp have been reported. 1D read have usually error rate about   10\%. (Check details!!, What is the error rate, how it's defined???) After parameter training, base calling can be performed by running the Viterbi algorithm, which will result in the sequence of states with the highest likelihood.

\chapter{Background}

\section{Sequencing overview}
Informally, the MinION sequencer works as follows. First, DNA is sheared into fragments of 8–20 kbp and adapters are ligated to either end of the fragments. The resulting DNA fragments pass through a protein embedded in a membrane via a nanometre-sized channel (this protein is the `nanopore'). A single strand of DNA passes through the pore; the optional use of a hairpin adapter at one end of the fragment allows the two strands of DNA to serially pass through nanopore, allowing two measurements of the fragment. With hairpin it's called 2D read, which isn't topic of this paper. In here we focus only on 1D reads.

In ONT terminology, the first strand going through the nanopore is the template, and the second is the complement. As a DNA strand passes through the pore it partially blocks the flow of electric current through the pore. The flow of current is sampled over time which is the observable output of the system. The central idea is that the single-stranded DNA product present in the nanopore affects the current in a way that is strong enough to enable decoding the electric signal data into a DNA sequence. This process, called basecalling, takes as input a list of current measurements, and produces as output a list of DNA bases most likely to have generated those currents.

The nanopore is 6 nucleoides wide, and eariler HMM based models model it as such.

\section{Basecalling}

The core of the decoding process is the basecalling step. Official basecaller is Metrichor, previously perfomed in the cloud before being opensourced under name Nanonet.

Earlier models were (Hidden markov model) HMM based where hidden state modeled DNA sequence of length 6 (6-mer) in the nanopore. Pore models were used in computing emission probabilities.~\cite{loman2015complete,schreiber2015analysis,szalay2015novo,timp2012dna} and the recent open source HMM based basecaller Nanocall~\cite{david2016nanocall}.

Recent models opted using (Recurrent neural network) RNN, notably DeepNano~\cite{deepnano} and recently open sourced official Nanonets from ONT.

\chapter{Method}

Instead of opting for traditional path using HMM or RNN we tried using CNN~(Convolutional neural networks)~\cite{lecun-98}, that is their residual version~\cite{he2016deep}. We opted out for gated residuan network variant~\cite{savarese2016learning}. For loss we used CTC (Connectionist temporal classification)~\cite{graves2006connectionist} between basecallled and target sequence. The implementation used is open source warp-ctc~\cite{warpctc}. Main computation framework is tensorflow~\cite{tensorflow2015-whitepaper}

\section{Data preprocessing}

Dataset were obtained from (Ask somebody, Marko doesn't know!). In this research models were trainined on ecoli K-12 strand. (Should I cite someone for strand). Data was split into train and test subset, such that aligned reads map to different reference genome parts. For initial model data bootstrapping metrichorn was used, version (Ask Someone?).

The fast5 input training files were further split into smaller training blocks, constiting of fixed block size on raw signal. For each block target sequence basecalled data is used in following way.

We're using basecalled knowledge which tells us on each raw read part which 6-mer were currently in the nanopore. Using this data, we get the intermediate target sequence for each block. To correct for model errors, we use aligned information, that is reference genome and alignment data(cigar string) to correct those information, yielding finalized target sequence. For further performace we skip first and last traning block, since most error aggregate on edges.

Adjacent matching bases are spearated with surogate one, for example: AAA -> AA'A for reasong described in following section.
\section{Residual arhitecture}

We use gated residual network, that is layer function is like this: $f(x) = k g(x) + x$ Where $x$ is input layer and $k$ constant learnt during training. Specifically $g$ used is multiple, 1--3 times in our experiments, Relu-BatchNorm-CNN layers.

\section{CTC}

This section gives brief overview over CTC, for further detail and in depth explaintion we recommend original paper~\cite{graves2006connectionist} or searhcing for contemporary blog posts.

In CTC we have target sequecne $\mathbf{l}$ consisting from symbols of alphabet $\Sigma$. Our models outputs $\mathbf{x}$, and CTC loss maximizes $p(\mathbf{l}|\mathbf{x})$.

$\mathbf{x}$ is consistend of $n$ independent discrete random variables, $X_i$  over domain $\Sigma \cup \text{Blank}$. Path $p$ through $\mathbf{x}$ is assigning for each $X_i$ single value. It's log probabity is $\log P(p) = \sum_i {\log P(X_i=x_i)}$ where $x_i$s define the path.

We further have merging operator on path, $\text{merge}(x_1, x_2, \ldots, x_n)$ which merges together adjacent equivalent elements. For example $\text{merge}(AAAGC) = AGC$.

Then $p(\mathbf{l}|\mathbf{x}) = \sum_{p\in\text{paths}(\mathbf{x}, \text{merged}(p) = \mathbf{l}}{P(p)}$ or less formally, $P(\mathbf{l}|\mathbf{x})$ is sum of all path probabilites which when merged give $\mathbf{l}$

In out concrete $x$ aplication $\Sigma = \{\text{A G T C A' G' T' A'}\}$

For decoding we use beam search decoder, with beam width 100.

\chapter{Results}

TODO

See \url{https://github.com/nmiculinic/minion-basecaller/blob/master/notebook/Lambda_nanonet_vs_m270.ipynb for preliminary results.}

\chapter{Conclusion and further work}

This model used advance state-of-the-art gated residual convolutional neural network, with top models having 270 layers and over 3M parameters, yet improvements over Metrichorn baseline are mariginal. As conclusion it might be that we've reached Bayesian error rate for R9 chemistry. Furthermore R9.5 and 1D\^2 reads are under development which shall yield this paper's result obsolite quite soon, yet underlying code developed could easily be adjusted, and trained on new data.

Unlike Nanonet which uses custom OpenCL kernels, or Albacore --- a novel ONT basecaller as of May 2017 lacking GPU support, this work used world class computational framework tensorflow with highly optimized kernels and large development community. Therefore resulting paper's effect is showcasing Residual CNN approach or pure CNN approach with CTC loss is marginally better than already established basecaller, and providing code in contemporary framework.

\bibliographystyle{fer}
\bibliography{refs}

\end{document}
