\documentclass[times, utf8, seminar]{fer}
\usepackage{booktabs}

\usepackage{mathtools, amsmath,amsfonts,amssymb, amsthm}
\usepackage{acronym}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {Figures/} }

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\begin{document}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\voditelj{Mile Sikić}
\title{CNNNano --- MinION  basecaller pogonjen dubokim ucenjem}
\engtitle{CNNNano --- MinION deep learning powered basecaller}
\author{Neven Miculinić}

\maketitle
\tableofcontents

\begin{sazetak}
TODO


\kljucnerijeci{TODO}
\end{sazetak}

\begin{abstract}
    The MinION device by Oxford Nanopore is the first portable sequencing device. MinION is able to produce very long reads (reads over 100 kBp were reported), however it suffers from high sequencing error rate. In this paper, we show novel model reducing the error rate can be reduced by improving the base calling process on 1D reads using gated residual convolutional neural networks (CNN) combined with Connectionist Temporal Classification (CTC) learning approach.

    Whole process is trainned end-to-end from raw signal on R9 sequecing pore and once trainned it's independent from Metrichor.
\keywords{}
\end{abstract}

\chapter{Introduction}

In this paper, we introduce the base caller for the MinION nanopore sequencing platform~\cite{mikheyev2014first}.The MinION device by Oxford Nanopore, weighing only 90 grams, is currently the smallest high-throughput DNA sequencer. Thanks to its low capital costs, small size and the possibility of analyzing the data in real time as they are produced, MinION is very promising for clinical applications, such as monitoring infectious disease outbreaks~\cite{judge2015early}\cite{quick2016real}, characterizing structural variants in cancer\cite{norris2016nanopore} and even full human genome assembly~\cite{jain2017nanopore}.

Although MinION is able to produce long reads, they have a high sequencing error rate. This has been somewhat aleviated with new R9 pore model, replacing older R7 ones. In this paper, we show that this error rate can be reduced by replacing the default base caller provided by the manufacturer with a properly trained neural network model.

In the MinION device, single-stranded DNA fragments move through nanopores, which causes drops in the electric current. The electric current is measured at each pore several thousand times per second. The electric current depends mostly on the context of several DNA bases passing through the pore at the time of measurement. As the DNA moves through the pore, the context shifts and the electric current changes.

A MinION device typically yields reads several thousand bases long; reads as long as 100,000 bp have been reported. 1D read have usually error rate about   30\%. After parameter training, base calling can be performed by running the Viterbi algorithm, which will result in the sequence of states with the highest likelihood.

\chapter{Background}

\section{Sequencing overview}
Informally, the MinION sequencer works as follows. First, DNA is sheared into fragments of 8–20 kbp and adapters are ligated to either end of the fragments. The resulting DNA fragments pass through a protein embedded in a membrane via a nanometre-sized channel (this protein is the `nanopore'). A single strand of DNA passes through the pore; the optional use of a hairpin adapter at one end of the fragment allows the two strands of DNA to serially pass through nanopore, allowing two measurements of the fragment. With hairpin it's called 2D read, which isn't topic of this paper. In here we focus only on 1D reads.

In ONT terminology, the first strand going through the nanopore is the template, and the second is the complement. As a DNA strand passes through the pore it partially blocks the flow of electric current through the pore. The flow of current is sampled over time which is the observable output of the system. The central idea is that the single-stranded DNA product present in the nanopore affects the current in a way that is strong enough to enable decoding the electric signal data into a DNA sequence. This process, called basecalling, takes as input a list of current measurements, and produces as output a list of DNA bases most likely to have generated those currents.

The nanopore is 6 nucleoides wide, and eariler HMM based models model it as such.

\section{Basecalling}

The core of the decoding process is the basecalling step. Official basecaller is Metrichor, previously perfomed in the cloud before being opensourced under name Nanonet.

Earlier models were (Hidden markov model) HMM based where hidden state modeled DNA sequence of length 6 (6-mer) in the nanopore. Pore models were used in computing emission probabilities.~\cite{loman2015complete,schreiber2015analysis,szalay2015novo,timp2012dna} and the recest open source HMM based basecaller Nanocall~\cite{david2016nanocall}.

Recent models opted using (Recurrent neural network) RNN, notably DeepNano~\cite{deepnano} and recently open sourced official Nanonets from ONT.

\chapter{Method}

Instead of opting for traditional path using HMM or RNN we tried using CNN~(Convolutional neural networks)~\cite{lecun-98}, that is their residual version~\cite{he2016deep}. We opted out for gated residuan network variant~\cite{savarese2016learning}. For loss we used CTC (Connectionist temporal classification)~\cite{graves2006connectionist} between basecallled and target sequence. The implementation used is open source warp-ctc~\cite{warpctc}. Main computation framework is tensorflow~\cite{tensorflow2015-whitepaper}

\section{Data preprocessing}

Dataset were obtained from (Ask Marko!). In this research models were trainined on ecoli K-12 strand. (Should I cite someone for strand). Data was split into train and test subset, such that aligned reads map to different reference genome parts. For initial model data bootstrapping metrichorn was used, version (Ask Someone?).

The fast5 input training files were further split into smaller training blocks, constiting of fixed block size on raw signal. For each block target sequence basecalled data is used in following way.

We're using basecalled knowledge which tells us on each raw read part which 6-mer were currently in the nanopore. Using this data, we get the intermediate target sequence for each block. To correc for model errors, we use aligned information, that is reference genome and alignment data(cigar string) to correct those information, yielding finalized target sequence. For further performace we skip first and last traning block, since most error aggregate on edges.

Adjacent matching bases are spearated with surogate one, for example: AAA -> AA'A for reasong described in following section.

\section{Residual arhitecture}

We use gated residual network, that is layer function is like this: $f(x) = k g(x) + x$ Where $x$ is input layer and $k$ constant learnt during training. Specifically $g$ used is multiple, 1--3 times in our experiments, Relu-BatchNorm-CNN layers.

\section{CTC}

This section gives brief overview over CTC, for further detail and in depth explaintion we recommend original paper~\cite{graves2006connectionist} or searhcing for contemporary blog posts.

In CTC we have target sequecne $\mathbf{l}$ consisting from symbols of alphabet $\Sigma$. Our models outputs $\mathbf{x}$, and CTC maximizes $p(\mathbf{l}|\mathbf{x})$.

$\mathbf{x}$ is consistend of $n$ independent discrete random variables, $X_i$  over domain $\Sigma \cup \text{Blank}$. Path $p$ through $\mathbf{x}$ is assigning for each $X_i$ single value. It's log probabity is $\log P(p) = \sum_i {\log P(X_i=x_i)}$ where $x_i$s define the path.

We further have merging operator on path, $\text{merge}(x_1, x_2, \ldots, x_n)$ which merges together adjacent equivalent elements. Then $p(\mathbf{l}|\mathbf{x}) = \sum_{p\in\text{paths}(\mathbf{x}, \text{merged}(p) = \mathbf{l}}{P(p)}$ or less formally, $P(\mathbf{l}|\mathbf{x})$ is sum of all path probabilites which when merged give $\mathbf{l}$

In out concrete $x$ aplication $\Sigma = \{\text{A G T C A' G' T' A'}\}$

For decoding we use beam search decoder, with beam width 100.

\chapter{Results}

Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\chapter{Conclusion and further work}
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
% \cite{deepnano}
% \bibliographystyle{fer}
% \bibliographystyle{abbrv}
% \bibliography{refs}

\end{document}
