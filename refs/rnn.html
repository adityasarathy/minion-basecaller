<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="small-crash-course-to-rnn-in-tensorflow">Small crash course to RNN in tensorflow</h1>
<p>Basic RNN have the following structure, where: * <span class="math inline">\(x_t\)</span> input at time <span class="math inline">\(t\)</span> * <span class="math inline">\(h_t\)</span> hidden state at time <span class="math inline">\(t\)</span> * <span class="math inline">\(\phi\)</span> activation function, usually <span class="math inline">\(\tanh\)</span></p>
<p><span class="math inline">\(h_t\)</span> is usually propagated to upper layers as output.</p>
<p>Usually beginning hidden state is initialized with zeros, that is <span class="math inline">\(h_0 = \mathbf{0}\)</span></p>
<p>Equations for vanilla RNN are as follows:</p>
<p><span class="math display">\[
\begin{align}
    h_t &amp;=\phi(Wh_{t_1} + Ux_t + b) \\
\end{align}
\]</span></p>
<p>The problem is simply vanishing/exploding gradients since with back propagation through time (BPTT) and information morphing (We apply non-linear transformation to hidden state each time step and the network cannot rely on same information representation in later time step). Exploding/Vanishing gradients are due to constant multiplication with matrix <span class="math inline">\(W\)</span> and is further explained <a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#a-mathematically-sufficient-condition-for-vanishing-sensitivity">here</a></p>
<p>Therefore, let's go furher to basic LSTM(Long short term memory) cell (I'm going to skip prototype LSTM cell which isn't working satisfactory due to unbounded state and other issues. See <a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#gluing-gates-together-to-derive-a-prototype-lstm">here</a>)</p>
<p>Basic LSTM cell as implemented in tf is in class <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#BasicLSTMCell">tf.nn.rnn_cell.BasicLSTMCell</a>. The internal architecture is as follows:</p>
<ul class="incremental">
<li><span class="math inline">\(h_t\)</span> - shadow state</li>
<li><span class="math inline">\(c_t\)</span> - cell</li>
<li><span class="math inline">\(i_t\)</span> - input gate</li>
<li><span class="math inline">\(o_t\)</span> - output gate</li>
<li><span class="math inline">\(f_t\)</span> - forget gate</li>
</ul>
<p><span class="math display">\[
\begin{align}
    i_t &amp;= \sigma(W_ih_{t-1} + U_i x_{t-1} + b_i) \\
    o_t &amp;= \sigma(W_oh_{t-1} + U_o x_{t-1} + b_o) \\
    f_t &amp;= \sigma(W_fh_{t-1} + U_f x_{t-1} + b_f) \\
    \\
    \tilde{c_t} &amp;= \phi(W_ch_{t-1} + U_cx_t + b_c) \\
    c_t &amp;= f_t \odot c_t + i_t \odot \tilde{c_t} \\
    \\
    h_t &amp;= o_t \odot \phi(c_t)\\
\end{align}
\]</span></p>
<p>Other altrenative is GRU (Gated Recurrent Unit) which has similar architecture as LSTM, though input and forget gates are merged, that is linked together. As inplemented in <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#GRUCell">GRUCell</a>:</p>
<ul class="incremental">
<li><span class="math inline">\(z_t\)</span> - update gate</li>
<li><span class="math inline">\(r_t\)</span> - reset gate</li>
<li><span class="math inline">\(\tilde{h_t}\)</span> - candidate write</li>
</ul>
<p><span class="math display">\[
\begin{align}
    r_t &amp;= \sigma(W_rh_{t-1} + U_r x_{t-1} + b_r) \\
    z_t &amp;= \sigma(W_zh_{t-1} + U_z x_{t-1} + b_z) \\
    \tilde{h_t} &amp;=  \phi(W_h (h_{t-1} \odot r_t) + U_h x_t + b_h)\\
    h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h_t}
\end{align}
\]</span></p>
<h1 id="tensorflow-implementation-details">Tensorflow implementation details</h1>
<p>For merging single RNN cell into multilayer RNN use <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#MultiRNNCell">MultiRNNCell</a>.</p>
<p>For dropout (on input and output, never on the state itself!) use <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell.html#DropoutWrapper">DropoutWrapper</a></p>
<p>For constructing RNN from cells (either MultiRNNCell, or any other combination) use <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#dynamic_rnn">tf.nn.dynamic_rnn</a>. It constructs computation graph dynamically at runtime and it's faster than tf.nn.rnn. I want to bring extra attention on parameter sequence_length:</p>
<pre><code>The parameter sequence_length is optional and is used to copy-through state and zero-out outputs when past a batch element&#39;s sequence length. So it&#39;s more for correctness than performance, unlike in rnn().</code></pre>
<p>As far as I figured out, we supply already max_time truncated samples into and it performs training starting from same inital state...I'm not really sure how to go when we have long sequence which we need to use BPTT...I mean they are ....todo</p>
<p><a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#state_saving_rnn">state_saving_rnn</a>... looks useful but I cannot really understand what's going on under the hood. After some googling I found <a href="https://www.tensorflow.org/versions/master/api_docs/python/contrib.training.html#SequenceQueueingStateSaver">this</a> as example usage.. skip for now.</p>
<p>There's few more useful looking functions, have a look <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#recurrent-neural-networks">here</a></p>
<p>In <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py">seq2seq</a> there's useful looking stuff, sequence_loss_by_example and potentially useful function.</p>
<h1 id="sweet-sweet-code">Sweet sweet code!!!</h1>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co">#!/usr/bin/evn python3</span>

<span class="im">import</span> tensorflow <span class="im">as</span> tf
<span class="im">import</span> numpy <span class="im">as</span> np

num_steps <span class="op">=</span> <span class="dv">32</span>
batch_size <span class="op">=</span> <span class="dv">32</span>
num_units <span class="op">=</span> <span class="dv">100</span>
num_layers <span class="op">=</span> <span class="dv">3</span>

X <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, num_steps])
Y <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, num_steps])

cell <span class="op">=</span> tf.nn.rnn_cell.GRUCell(num_units)
cell <span class="op">=</span> tf.nn.rnn_cell.MultiRNNCell([cell] <span class="op">*</span> num_layers)


init_state <span class="op">=</span> cell.zero_state(batch_size, tf.float32)
rnn_outputs, final_state <span class="op">=</span> tf.nn.dynamic_rnn(cell, X, initial_state<span class="op">=</span>init_state)

<span class="cf">with</span> tf.variable_scope(<span class="st">&#39;softmax&#39;</span>):
    W <span class="op">=</span> tf.get_variable(<span class="st">&#39;W&#39;</span>, [num_units, num_classes])
    b <span class="op">=</span> tf.get_variable(<span class="st">&#39;b&#39;</span>, [num_classes], initializer<span class="op">=</span>tf.constant_initializer(<span class="fl">0.0</span>))

<span class="co"># A bit of reshaping magic</span>
rnn_outputs <span class="op">=</span> tf.reshape(rnn_outputs, [<span class="op">-</span><span class="dv">1</span>, state_size])
y_reshaped <span class="op">=</span> tf.reshape(y, [<span class="op">-</span><span class="dv">1</span>])

logits <span class="op">=</span> tf.matmul(rnn_outputs, W) <span class="op">+</span> b

predictions <span class="op">=</span> tf.nn.softmax(logits)

total_loss <span class="op">=</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))
train_step <span class="op">=</span> tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</code></pre></div>
<p>During traning, lets assume we have long sequences x, y, and we already subdivided them in smaller parts with size [batch_size, num_steps]. Then we train as:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">training_state <span class="op">=</span> <span class="va">None</span>
<span class="cf">for</span> x,y <span class="kw">in</span> <span class="bu">zip</span>(X, Y):
    feed_dict<span class="op">=</span>{g[<span class="st">&#39;x&#39;</span>]: X, g[<span class="st">&#39;y&#39;</span>]: Y}
    <span class="cf">if</span> training_state <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:
        feed_dict[g[<span class="st">&#39;init_state&#39;</span>]] <span class="op">=</span> training_state
    _, sess.run([train_step, final_state], feed_dict)</code></pre></div>
<h1 id="other-resources">Other resources</h1>
<ul class="incremental">
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah - Understanding LSTM Networks</a> - quick and dirty overview of basic RNN cells</li>
<li><a href="http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf">Training recurrent neural network, Ilya Sutskever 2013 phd thesis</a></li>
<li><a href="http://www.deeplearningbook.org/contents/rnn.html">RNN chapter in Deep Learning Book</a></li>
<li><a href="https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html">Official tf RNN tutorial</a></li>
<li><a href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/">RNN in tensorflow - a practical guide and undocumented features</a> -- tf documentation could be better with more examples, so I have to scrape everything from blogs mostly</li>
<li><a href="http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">R2RT - Written Memories: Understanding, Deriving and Extending the LSTM</a> -- excellent blog post with lot of math in it</li>
<li><a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html">R2RT - Recurrent Neural Networks in Tensorflow I</a> and <a href="http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html">part 2</a> -- good resources with lot of practical tips, code and up to date constructs</li>
<li><a href="http://r2rt.com/styles-of-truncated-backpropagation.html">Styles of Truncated Backpropagation</a> -- explains styles of BPTT and which one tensorflow uses</li>
</ul>
</body>
</html>
